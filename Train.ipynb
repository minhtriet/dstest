{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d49b21",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.1-cp310-cp310-win_amd64.whl (9.3 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting scipy>=1.5.0\n",
      "  Using cached scipy-1.11.3-cp310-cp310-win_amd64.whl (44.1 MB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.1 scipy-1.11.3 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759e1262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\Train.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/Train.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, DataCollatorWithPadding\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/Train.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForSequenceClassification, TrainingArguments, Trainer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/Train.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m log_loss\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/Train.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/Train.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from helper import preprocess_df, preprocess_text\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a865260",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9d586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>box</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "      <th>linking</th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[394, 145, 433, 162]</td>\n",
       "      <td>ITEM:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [394, 145, 433, 162], 'text': 'ITEM:'}]</td>\n",
       "      <td>[[0, 15]]</td>\n",
       "      <td>0</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[109, 112, 151, 129]</td>\n",
       "      <td>DATE:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [109, 112, 151, 129], 'text': 'DATE:'}]</td>\n",
       "      <td>[[1, 13]]</td>\n",
       "      <td>1</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[110, 140, 159, 155]</td>\n",
       "      <td>BRAND:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [110, 140, 159, 155], 'text': 'BRAND:'}]</td>\n",
       "      <td>[[2, 14]]</td>\n",
       "      <td>2</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[109, 183, 168, 198]</td>\n",
       "      <td>SUMMARY</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [109, 183, 168, 198], 'text': 'SUMMAR...</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[175, 184, 193, 195]</td>\n",
       "      <td>OF</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [175, 184, 193, 195], 'text': 'OF'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[201, 184, 267, 198]</td>\n",
       "      <td>PROJECT:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [201, 184, 267, 198], 'text': 'PROJEC...</td>\n",
       "      <td>[[5, 7], [5, 6]]</td>\n",
       "      <td>5</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[317, 186, 385, 199]</td>\n",
       "      <td>Attached</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'box': [317, 186, 385, 199], 'text': 'Attach...</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>6</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[282, 186, 310, 197]</td>\n",
       "      <td>See</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'box': [282, 186, 310, 197], 'text': 'See'}]</td>\n",
       "      <td>[[5, 7]]</td>\n",
       "      <td>7</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[103, 404, 173, 418]</td>\n",
       "      <td>FUNDING:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [103, 404, 173, 418], 'text': 'FUNDIN...</td>\n",
       "      <td>[[8, 22], [8, 23]]</td>\n",
       "      <td>8</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[101, 559, 196, 574]</td>\n",
       "      <td>SIGNATURES:</td>\n",
       "      <td>header</td>\n",
       "      <td>[{'box': [101, 559, 196, 574], 'text': 'SIGNAT...</td>\n",
       "      <td>[[9, 24], [9, 25], [9, 26], [9, 27], [9, 28]]</td>\n",
       "      <td>9</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    box         text     label  \\\n",
       "0  [394, 145, 433, 162]        ITEM:  question   \n",
       "1  [109, 112, 151, 129]        DATE:  question   \n",
       "2  [110, 140, 159, 155]       BRAND:  question   \n",
       "3  [109, 183, 168, 198]      SUMMARY  question   \n",
       "4  [175, 184, 193, 195]           OF  question   \n",
       "5  [201, 184, 267, 198]     PROJECT:  question   \n",
       "6  [317, 186, 385, 199]     Attached    answer   \n",
       "7  [282, 186, 310, 197]          See    answer   \n",
       "8  [103, 404, 173, 418]     FUNDING:  question   \n",
       "9  [101, 559, 196, 574]  SIGNATURES:    header   \n",
       "\n",
       "                                               words  \\\n",
       "0   [{'box': [394, 145, 433, 162], 'text': 'ITEM:'}]   \n",
       "1   [{'box': [109, 112, 151, 129], 'text': 'DATE:'}]   \n",
       "2  [{'box': [110, 140, 159, 155], 'text': 'BRAND:'}]   \n",
       "3  [{'box': [109, 183, 168, 198], 'text': 'SUMMAR...   \n",
       "4      [{'box': [175, 184, 193, 195], 'text': 'OF'}]   \n",
       "5  [{'box': [201, 184, 267, 198], 'text': 'PROJEC...   \n",
       "6  [{'box': [317, 186, 385, 199], 'text': 'Attach...   \n",
       "7     [{'box': [282, 186, 310, 197], 'text': 'See'}]   \n",
       "8  [{'box': [103, 404, 173, 418], 'text': 'FUNDIN...   \n",
       "9  [{'box': [101, 559, 196, 574], 'text': 'SIGNAT...   \n",
       "\n",
       "                                         linking  id      file  \n",
       "0                                      [[0, 15]]   0  71206427  \n",
       "1                                      [[1, 13]]   1  71206427  \n",
       "2                                      [[2, 14]]   2  71206427  \n",
       "3                                             []   3  71206427  \n",
       "4                                             []   4  71206427  \n",
       "5                               [[5, 7], [5, 6]]   5  71206427  \n",
       "6                                       [[5, 6]]   6  71206427  \n",
       "7                                       [[5, 7]]   7  71206427  \n",
       "8                             [[8, 22], [8, 23]]   8  71206427  \n",
       "9  [[9, 24], [9, 25], [9, 26], [9, 27], [9, 28]]   9  71206427  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7fd9a",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = preprocess_text(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b79a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "box        0\n",
       "text       0\n",
       "label      0\n",
       "words      0\n",
       "linking    0\n",
       "id         0\n",
       "file       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(subset='text').isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a814b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e995a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data, test_size=0.5, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cafe5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>box</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "      <th>linking</th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>[175, 556, 321, 570]</td>\n",
       "      <td>sp sneak preview</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'text': 'SP', 'box': [175, 556, 195, 569]}, ...</td>\n",
       "      <td>[[44, 46]]</td>\n",
       "      <td>46</td>\n",
       "      <td>00920294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>[22, 490, 117, 506]</td>\n",
       "      <td>fowlers oil (308 -21)</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'text': 'Fowlers', 'box': [22, 490, 60, 505]...</td>\n",
       "      <td>[[48, 61]]</td>\n",
       "      <td>61</td>\n",
       "      <td>81619511_9513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5301</th>\n",
       "      <td>[532, 52, 670, 67]</td>\n",
       "      <td>submission date:</td>\n",
       "      <td>header</td>\n",
       "      <td>[{'text': 'SUBMISSION', 'box': [532, 54, 624, ...</td>\n",
       "      <td>[[9, 13], [9, 14], [9, 15], [9, 16]]</td>\n",
       "      <td>9</td>\n",
       "      <td>92657311_7313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>[156, 419, 203, 447]</td>\n",
       "      <td>ind. lor volume</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'text': 'IND.', 'box': [156, 422, 178, 433]}...</td>\n",
       "      <td>[[49, 54], [49, 55], [49, 56], [49, 57], [49, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>81619511_9513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>[265, 218, 453, 236]</td>\n",
       "      <td>sample no. 1166, rd 308</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'text': 'Sample', 'box': [265, 219, 312, 236...</td>\n",
       "      <td>[[9, 10]]</td>\n",
       "      <td>10</td>\n",
       "      <td>01197604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>[141, 110, 654, 128]</td>\n",
       "      <td>diethyl 3, 3-dimethyl-2-oxo-1, 4-cyclopentaned...</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'text': 'Diethyl', 'box': [141, 112, 207, 12...</td>\n",
       "      <td>[[0, 71]]</td>\n",
       "      <td>71</td>\n",
       "      <td>01073843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>[52, 364, 131, 381]</td>\n",
       "      <td>projected:</td>\n",
       "      <td>header</td>\n",
       "      <td>[{'box': [52, 364, 131, 381], 'text': 'Project...</td>\n",
       "      <td>[[2, 44], [2, 45], [2, 46], [2, 47]]</td>\n",
       "      <td>2</td>\n",
       "      <td>0012529295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>[81, 168, 245, 183]</td>\n",
       "      <td>(4) product length:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'text': '(4)', 'box': [81, 168, 106, 183]}, ...</td>\n",
       "      <td>[[26, 4], [22, 26]]</td>\n",
       "      <td>26</td>\n",
       "      <td>0060308461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>[80, 873, 194, 886]</td>\n",
       "      <td>xer 264/ rev 5/ 68</td>\n",
       "      <td>other</td>\n",
       "      <td>[{'text': 'XER', 'box': [80, 873, 102, 886]}, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>28</td>\n",
       "      <td>0011859695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>[630, 155, 651, 166]</td>\n",
       "      <td>no.</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [630, 155, 651, 166], 'text': 'NO.'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>00836244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3624 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       box                                               text  \\\n",
       "2899  [175, 556, 321, 570]                                   sp sneak preview   \n",
       "203    [22, 490, 117, 506]                              fowlers oil (308 -21)   \n",
       "5301    [532, 52, 670, 67]                                   submission date:   \n",
       "191   [156, 419, 203, 447]                                    ind. lor volume   \n",
       "3629  [265, 218, 453, 236]                            sample no. 1166, rd 308   \n",
       "...                    ...                                                ...   \n",
       "3048  [141, 110, 654, 128]  diethyl 3, 3-dimethyl-2-oxo-1, 4-cyclopentaned...   \n",
       "2031   [52, 364, 131, 381]                                         projected:   \n",
       "1080   [81, 168, 245, 183]                                (4) product length:   \n",
       "4952   [80, 873, 194, 886]                                 xer 264/ rev 5/ 68   \n",
       "604   [630, 155, 651, 166]                                                no.   \n",
       "\n",
       "         label                                              words  \\\n",
       "2899    answer  [{'text': 'SP', 'box': [175, 556, 195, 569]}, ...   \n",
       "203     answer  [{'text': 'Fowlers', 'box': [22, 490, 60, 505]...   \n",
       "5301    header  [{'text': 'SUBMISSION', 'box': [532, 54, 624, ...   \n",
       "191   question  [{'text': 'IND.', 'box': [156, 422, 178, 433]}...   \n",
       "3629    answer  [{'text': 'Sample', 'box': [265, 219, 312, 236...   \n",
       "...        ...                                                ...   \n",
       "3048    answer  [{'text': 'Diethyl', 'box': [141, 112, 207, 12...   \n",
       "2031    header  [{'box': [52, 364, 131, 381], 'text': 'Project...   \n",
       "1080  question  [{'text': '(4)', 'box': [81, 168, 106, 183]}, ...   \n",
       "4952     other  [{'text': 'XER', 'box': [80, 873, 102, 886]}, ...   \n",
       "604   question     [{'box': [630, 155, 651, 166], 'text': 'NO.'}]   \n",
       "\n",
       "                                                linking  id           file  \n",
       "2899                                         [[44, 46]]  46       00920294  \n",
       "203                                          [[48, 61]]  61  81619511_9513  \n",
       "5301               [[9, 13], [9, 14], [9, 15], [9, 16]]   9  92657311_7313  \n",
       "191   [[49, 54], [49, 55], [49, 56], [49, 57], [49, ...  49  81619511_9513  \n",
       "3629                                          [[9, 10]]  10       01197604  \n",
       "...                                                 ...  ..            ...  \n",
       "3048                                          [[0, 71]]  71       01073843  \n",
       "2031               [[2, 44], [2, 45], [2, 46], [2, 47]]   2     0012529295  \n",
       "1080                                [[26, 4], [22, 26]]  26     0060308461  \n",
       "4952                                                 []  28     0011859695  \n",
       "604                                                  []   8       00836244  \n",
       "\n",
       "[3624 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d98a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>box</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "      <th>linking</th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[394, 145, 433, 162]</td>\n",
       "      <td>item:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [394, 145, 433, 162], 'text': 'ITEM:'}]</td>\n",
       "      <td>[[0, 15]]</td>\n",
       "      <td>0</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[109, 112, 151, 129]</td>\n",
       "      <td>date:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [109, 112, 151, 129], 'text': 'DATE:'}]</td>\n",
       "      <td>[[1, 13]]</td>\n",
       "      <td>1</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[110, 140, 159, 155]</td>\n",
       "      <td>brand:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [110, 140, 159, 155], 'text': 'BRAND:'}]</td>\n",
       "      <td>[[2, 14]]</td>\n",
       "      <td>2</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[109, 183, 168, 198]</td>\n",
       "      <td>summary</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [109, 183, 168, 198], 'text': 'SUMMAR...</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[175, 184, 193, 195]</td>\n",
       "      <td>of</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'box': [175, 184, 193, 195], 'text': 'OF'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>71206427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>[397, 881, 528, 914]</td>\n",
       "      <td>topline w/o 12 /18 /95 final report 12/ 31/ 95</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'text': 'Topline', 'box': [398, 883, 443, 89...</td>\n",
       "      <td>[[59, 60]]</td>\n",
       "      <td>60</td>\n",
       "      <td>80310840a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7407</th>\n",
       "      <td>[577, 742, 635, 759]</td>\n",
       "      <td>$100 600</td>\n",
       "      <td>other</td>\n",
       "      <td>[{'text': '$100', 'box': [577, 742, 606, 757]}...</td>\n",
       "      <td>[]</td>\n",
       "      <td>61</td>\n",
       "      <td>80310840a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7408</th>\n",
       "      <td>[398, 761, 560, 779]</td>\n",
       "      <td>revised costs (if any)</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'text': '', 'box': [398, 761, 409, 774]}, {'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>62</td>\n",
       "      <td>80310840a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7409</th>\n",
       "      <td>[397, 788, 532, 806]</td>\n",
       "      <td>fieldwork schedule:</td>\n",
       "      <td>question</td>\n",
       "      <td>[{'text': 'Fieldwork', 'box': [397, 788, 464, ...</td>\n",
       "      <td>[[63, 58]]</td>\n",
       "      <td>63</td>\n",
       "      <td>80310840a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7410</th>\n",
       "      <td>[204, 642, 295, 659]</td>\n",
       "      <td>a &amp; g research</td>\n",
       "      <td>answer</td>\n",
       "      <td>[{'text': 'A', 'box': [204, 642, 214, 657]}, {...</td>\n",
       "      <td>[[47, 64]]</td>\n",
       "      <td>64</td>\n",
       "      <td>80310840a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7248 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       box                                            text  \\\n",
       "0     [394, 145, 433, 162]                                           item:   \n",
       "1     [109, 112, 151, 129]                                           date:   \n",
       "2     [110, 140, 159, 155]                                          brand:   \n",
       "3     [109, 183, 168, 198]                                         summary   \n",
       "4     [175, 184, 193, 195]                                              of   \n",
       "...                    ...                                             ...   \n",
       "7406  [397, 881, 528, 914]  topline w/o 12 /18 /95 final report 12/ 31/ 95   \n",
       "7407  [577, 742, 635, 759]                                        $100 600   \n",
       "7408  [398, 761, 560, 779]                          revised costs (if any)   \n",
       "7409  [397, 788, 532, 806]                             fieldwork schedule:   \n",
       "7410  [204, 642, 295, 659]                                  a & g research   \n",
       "\n",
       "         label                                              words     linking  \\\n",
       "0     question   [{'box': [394, 145, 433, 162], 'text': 'ITEM:'}]   [[0, 15]]   \n",
       "1     question   [{'box': [109, 112, 151, 129], 'text': 'DATE:'}]   [[1, 13]]   \n",
       "2     question  [{'box': [110, 140, 159, 155], 'text': 'BRAND:'}]   [[2, 14]]   \n",
       "3     question  [{'box': [109, 183, 168, 198], 'text': 'SUMMAR...          []   \n",
       "4     question      [{'box': [175, 184, 193, 195], 'text': 'OF'}]          []   \n",
       "...        ...                                                ...         ...   \n",
       "7406    answer  [{'text': 'Topline', 'box': [398, 883, 443, 89...  [[59, 60]]   \n",
       "7407     other  [{'text': '$100', 'box': [577, 742, 606, 757]}...          []   \n",
       "7408  question  [{'text': '', 'box': [398, 761, 409, 774]}, {'...          []   \n",
       "7409  question  [{'text': 'Fieldwork', 'box': [397, 788, 464, ...  [[63, 58]]   \n",
       "7410    answer  [{'text': 'A', 'box': [204, 642, 214, 657]}, {...  [[47, 64]]   \n",
       "\n",
       "      id       file  \n",
       "0      0   71206427  \n",
       "1      1   71206427  \n",
       "2      2   71206427  \n",
       "3      3   71206427  \n",
       "4      4   71206427  \n",
       "...   ..        ...  \n",
       "7406  60  80310840a  \n",
       "7407  61  80310840a  \n",
       "7408  62  80310840a  \n",
       "7409  63  80310840a  \n",
       "7410  64  80310840a  \n",
       "\n",
       "[7248 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e58686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a small dataset, therefore I save everything in memory\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, texts, labels):\n",
    "        assert len(texts) == len(labels)\n",
    "        self.index_label = dict(zip(range(4), ['answer', 'header', 'other', 'question']))\n",
    "        self.label_index = dict(zip(['answer', 'header', 'other', 'question'], range(4)))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        for text in self.texts:\n",
    "            _ = tokenizer(text, truncation=True)\n",
    "            input_id, attn_mask = _['input_ids'], _['attention_mask']\n",
    "            self.input_ids.append(input_id)\n",
    "            self.attn_masks.append(attn_mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {#\"text\": self.tokenizer.tokenize(self.texts[idx]), \n",
    "        \"label\": self.label_index[self.labels[idx]], \n",
    "        'input_ids': self.input_ids[idx], \n",
    "        'attention_mask': self.attn_masks[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(tokenizer, train_df['text'].values, train_df['label'].values)\n",
    "val_dataset = TextDataset(tokenizer, val_df['text'].values, val_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd47864",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_dataset)+len(val_dataset)==len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88198aa6",
   "metadata": {},
   "source": [
    "## Define loss function\n",
    "Since `other` and `header` are under-represented, we will add more weight to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_label = dict(zip(range(4), ['answer', 'header', 'other', 'question']))\n",
    "label_index = dict(zip(['answer', 'header', 'other', 'question'], range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo whole dataset count label, inverse porpotional, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa69eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels_dict = data['label'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa743f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2721"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_labels_dict['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e2402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.376417233560091"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0022675736961451248/0.0003074085459575776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd95ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before {'question': 3253, 'answer': 2721, 'other': 833, 'header': 441}\n",
      "After {'question': 0.0003074085459575776, 'answer': 0.0003675119441381845, 'other': 0.0012004801920768306, 'header': 0.0022675736961451248}\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\", count_labels_dict)\n",
    "for key in count_labels_dict.keys():\n",
    "    # inverse\n",
    "    count_labels_dict[key] = 1/count_labels_dict[key]\n",
    "print(\"After\", count_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = torch.Tensor([count_labels_dict['answer'],count_labels_dict['header'],\n",
    "                              count_labels_dict['other'],count_labels_dict['question']])\n",
    "weight_tensor = torch.Tensor([1,1.1,1.1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2eaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = self.loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd0b10",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d9653",
   "metadata": {},
   "source": [
    "### Hyperparams tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52cc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=4, id2label=index_label, label2id=label_index,\n",
    "    problem_type='single_label_classification'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c47c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-3, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 1e-1, log=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a3696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/chaum/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"answer\",\n",
      "    \"1\": \"header\",\n",
      "    \"2\": \"other\",\n",
      "    \"3\": \"question\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"answer\": 0,\n",
      "    \"header\": 1,\n",
      "    \"other\": 2,\n",
      "    \"question\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/chaum/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    model_init=model_init\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-12 17:22:24,692]\u001b[0m A new study created in memory with name: no-name-147a096c-402b-40ce-9eac-356a12d73e33\u001b[0m\n",
      "Trial: {'learning_rate': 0.00016176775440442069, 'per_device_train_batch_size': 16, 'weight_decay': 0.025364953529501567}\n",
      "loading configuration file config.json from cache at /Users/chaum/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"answer\",\n",
      "    \"1\": \"header\",\n",
      "    \"2\": \"other\",\n",
      "    \"3\": \"question\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"answer\": 0,\n",
      "    \"header\": 1,\n",
      "    \"other\": 2,\n",
      "    \"question\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/chaum/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/chaum/code/dstest/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3624\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 681\n",
      "  Number of trainable parameters = 66956548\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='681' max='681' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [681/681 3:02:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.648360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.670285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.741448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/run-0/checkpoint-500\n",
      "Configuration saved in models/run-0/checkpoint-500/config.json\n",
      "Model weights saved in models/run-0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in models/run-0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in models/run-0/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-02-12 20:24:38,478]\u001b[0m Trial 0 finished with value: 0.7414484620094299 and parameters: {'learning_rate': 0.00016176775440442069, 'per_device_train_batch_size': 16, 'weight_decay': 0.025364953529501567}. Best is trial 0 with value: 0.7414484620094299.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    timeout=1000,\n",
    "    hp_space=hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84741314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.00016176775440442069,\n",
       " 'per_device_train_batch_size': 16,\n",
       " 'weight_decay': 0.025364953529501567}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial.hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330873b",
   "metadata": {},
   "source": [
    "### Train with best hyperparams in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = TextDataset(tokenizer, data['text'].values, data['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fc461",
   "metadata": {},
   "source": [
    "```\n",
    "{'learning_rate': 0.0008549921087552112,\n",
    " 'per_device_train_batch_size': 32,\n",
    " 'weight_decay': 0.08477673461329668}\n",
    "```\n",
    "With these hyperparams, we will train with all the data to predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ef91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# some values below are from previous hyperpams search\n",
    "best_training_args = TrainingArguments(\n",
    "    learning_rate=best_trial.hyperparameters['learning_rate'],  # 0.00022191984360513044\n",
    "    per_device_train_batch_size=best_trial.hyperparameters['per_device_train_batch_size'],   # 16\n",
    "    weight_decay=best_trial.hyperparameters['weight_decay'],    # 0.004611087956707392\n",
    "    num_train_epochs=15,\n",
    "    output_dir=\"models\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",    \n",
    "    save_total_limit = 3,\n",
    "    report_to=\"tensorboard\",\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef940068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/chaum/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"answer\",\n",
      "    \"1\": \"header\",\n",
      "    \"2\": \"other\",\n",
      "    \"3\": \"question\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"answer\": 0,\n",
      "    \"header\": 1,\n",
      "    \"other\": 2,\n",
      "    \"question\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/chaum/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "best_model = model_init(None)\n",
    "best_trainer = WeightedTrainer(\n",
    "    model=best_model,\n",
    "    args=best_training_args,\n",
    "    train_dataset=whole_dataset,  # not train_dataset anymore, as we will train on the whole data\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a47449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaum/code/dstest/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7248\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6795\n",
      "  Number of trainable parameters = 66956548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6795' max='6795' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6795/6795 45:15, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.677450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.609645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>0.578329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>0.475208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.429468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.377185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.345363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.434300</td>\n",
       "      <td>0.310338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.434300</td>\n",
       "      <td>0.311550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.255763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.280004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.266505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.221522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.222516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.241719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.234778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.199582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.190587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.186105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.171214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.175928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.154589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.155066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.146050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.132223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.151299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.127835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.122793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.111081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.109181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.106756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.102089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.101534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-200\n",
      "Configuration saved in models/checkpoint-200/config.json\n",
      "Model weights saved in models/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-400\n",
      "Configuration saved in models/checkpoint-400/config.json\n",
      "Model weights saved in models/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-600\n",
      "Configuration saved in models/checkpoint-600/config.json\n",
      "Model weights saved in models/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-800\n",
      "Configuration saved in models/checkpoint-800/config.json\n",
      "Model weights saved in models/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-1000\n",
      "Configuration saved in models/checkpoint-1000/config.json\n",
      "Model weights saved in models/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-1200\n",
      "Configuration saved in models/checkpoint-1200/config.json\n",
      "Model weights saved in models/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-1400\n",
      "Configuration saved in models/checkpoint-1400/config.json\n",
      "Model weights saved in models/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-1600\n",
      "Configuration saved in models/checkpoint-1600/config.json\n",
      "Model weights saved in models/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-1800\n",
      "Configuration saved in models/checkpoint-1800/config.json\n",
      "Model weights saved in models/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-2000\n",
      "Configuration saved in models/checkpoint-2000/config.json\n",
      "Model weights saved in models/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-2200\n",
      "Configuration saved in models/checkpoint-2200/config.json\n",
      "Model weights saved in models/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-2400\n",
      "Configuration saved in models/checkpoint-2400/config.json\n",
      "Model weights saved in models/checkpoint-2400/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-2600\n",
      "Configuration saved in models/checkpoint-2600/config.json\n",
      "Model weights saved in models/checkpoint-2600/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-2600/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-2800\n",
      "Configuration saved in models/checkpoint-2800/config.json\n",
      "Model weights saved in models/checkpoint-2800/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-2800/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-3000\n",
      "Configuration saved in models/checkpoint-3000/config.json\n",
      "Model weights saved in models/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-3200\n",
      "Configuration saved in models/checkpoint-3200/config.json\n",
      "Model weights saved in models/checkpoint-3200/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-3200/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-3200/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-3400\n",
      "Configuration saved in models/checkpoint-3400/config.json\n",
      "Model weights saved in models/checkpoint-3400/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-3400/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-3400/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-3600\n",
      "Configuration saved in models/checkpoint-3600/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/checkpoint-3600/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-3600/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-3800\n",
      "Configuration saved in models/checkpoint-3800/config.json\n",
      "Model weights saved in models/checkpoint-3800/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-3800/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-3800/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-4000\n",
      "Configuration saved in models/checkpoint-4000/config.json\n",
      "Model weights saved in models/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-4200\n",
      "Configuration saved in models/checkpoint-4200/config.json\n",
      "Model weights saved in models/checkpoint-4200/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-4200/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-4200/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-4400\n",
      "Configuration saved in models/checkpoint-4400/config.json\n",
      "Model weights saved in models/checkpoint-4400/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-4400/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-4400/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-4600\n",
      "Configuration saved in models/checkpoint-4600/config.json\n",
      "Model weights saved in models/checkpoint-4600/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-4600/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-4600/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-4800\n",
      "Configuration saved in models/checkpoint-4800/config.json\n",
      "Model weights saved in models/checkpoint-4800/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-4800/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-4800/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-4200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-5000\n",
      "Configuration saved in models/checkpoint-5000/config.json\n",
      "Model weights saved in models/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-4400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-5200\n",
      "Configuration saved in models/checkpoint-5200/config.json\n",
      "Model weights saved in models/checkpoint-5200/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-5200/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-5200/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-4600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-5400\n",
      "Configuration saved in models/checkpoint-5400/config.json\n",
      "Model weights saved in models/checkpoint-5400/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-5400/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-5400/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-4800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-5600\n",
      "Configuration saved in models/checkpoint-5600/config.json\n",
      "Model weights saved in models/checkpoint-5600/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-5600/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-5600/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-5800\n",
      "Configuration saved in models/checkpoint-5800/config.json\n",
      "Model weights saved in models/checkpoint-5800/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-5800/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-5800/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-5200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-6000\n",
      "Configuration saved in models/checkpoint-6000/config.json\n",
      "Model weights saved in models/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-5400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-6200\n",
      "Configuration saved in models/checkpoint-6200/config.json\n",
      "Model weights saved in models/checkpoint-6200/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-6200/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-6200/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-5600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-6400\n",
      "Configuration saved in models/checkpoint-6400/config.json\n",
      "Model weights saved in models/checkpoint-6400/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-6400/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-6400/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-5800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3624\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/checkpoint-6600\n",
      "Configuration saved in models/checkpoint-6600/config.json\n",
      "Model weights saved in models/checkpoint-6600/pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-6600/tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-6600/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-6000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/checkpoint-6600 (score: 0.10153426975011826).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6795, training_loss=0.2902216354830463, metrics={'train_runtime': 2716.1069, 'train_samples_per_second': 40.028, 'train_steps_per_second': 2.502, 'total_flos': 620772649722624.0, 'train_loss': 0.2902216354830463, 'epoch': 15.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929acb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/best\n",
      "Configuration saved in models/best/config.json\n",
      "Model weights saved in models/best/pytorch_model.bin\n",
      "tokenizer config file saved in models/best/tokenizer_config.json\n",
      "Special tokens file saved in models/best/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "best_trainer.save_model(\"models/best\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc56a23bccbe5aceb3052fbf4cfaff87eb4b620654d0523699a9ffb8cad4c286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TR question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This requires running tr_q1 first\n",
    "\n",
    "The order of the notebook is as follow, \n",
    "- Split train val test class\n",
    "- Define a baseline\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "seed = 42\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I take the headtext and firsts characters of each paragraph and concatenatem together, such that each row in the dataset are representend by `MAX_LENGTH` characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'TRDataChallenge2023.zip'\n",
    "extract_file_path = 'TRDataChallenge2023'\n",
    "df = pd.read_json(os.path.join(extract_file_path, f\"{extract_file_path}.txt\"), lines=True).head(10)   # todo\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = pd.Series(np.array(mlb.fit_transform(df[\"postures\"].values), dtype=\"float\").tolist(), name=\"label_ids\")\n",
    "df = pd.concat([df, labels], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentId</th>\n",
       "      <th>postures</th>\n",
       "      <th>sections</th>\n",
       "      <th>label_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ib4e590e0a55f11e8a5d58a2c8dcb28b5</td>\n",
       "      <td>[On Appeal]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Plaintiff Dw...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ib06ab4d056a011e98c7a8e995225dbf9</td>\n",
       "      <td>[Appellate Review, Sentencing or Penalty Phase...</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['After pleadi...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iaa3e3390b93111e9ba33b03ae9101fb2</td>\n",
       "      <td>[Motion to Compel Arbitration, On Appeal]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Frederick Gr...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I0d4dffc381b711e280719c3f0e80bdd0</td>\n",
       "      <td>[On Appeal, Review of Administrative Decision]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Appeal from ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I82c7ef10d6d111e8aec5b23c3317c9c0</td>\n",
       "      <td>[On Appeal]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Order, Supre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          documentId  \\\n",
       "0  Ib4e590e0a55f11e8a5d58a2c8dcb28b5   \n",
       "1  Ib06ab4d056a011e98c7a8e995225dbf9   \n",
       "2  Iaa3e3390b93111e9ba33b03ae9101fb2   \n",
       "3  I0d4dffc381b711e280719c3f0e80bdd0   \n",
       "4  I82c7ef10d6d111e8aec5b23c3317c9c0   \n",
       "\n",
       "                                            postures  \\\n",
       "0                                        [On Appeal]   \n",
       "1  [Appellate Review, Sentencing or Penalty Phase...   \n",
       "2          [Motion to Compel Arbitration, On Appeal]   \n",
       "3     [On Appeal, Review of Administrative Decision]   \n",
       "4                                        [On Appeal]   \n",
       "\n",
       "                                            sections  \\\n",
       "0  [{'headtext': '', 'paragraphs': ['Plaintiff Dw...   \n",
       "1  [{'headtext': '', 'paragraphs': ['After pleadi...   \n",
       "2  [{'headtext': '', 'paragraphs': ['Frederick Gr...   \n",
       "3  [{'headtext': '', 'paragraphs': ['Appeal from ...   \n",
       "4  [{'headtext': '', 'paragraphs': ['Order, Supre...   \n",
       "\n",
       "                        label_ids  \n",
       "0  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  \n",
       "1  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "2  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0]  \n",
       "3  [0.0, 0.0, 0.0, 1.0, 1.0, 0.0]  \n",
       "4  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we `fit_transform` in the train set and `transform` on the test set. However here I `fit_transform` in the whole dataset to cover all of the labels, because some of them only have one instance (See first notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_strings(max_len, sections):\n",
    "    \"\"\"\n",
    "    Remove the \\\\u and clean up texts\n",
    "    \"\"\"\n",
    "    cleaned_sections = []\n",
    "    chars_per_section = max_len // len(sections)\n",
    "    for section in sections:            \n",
    "        cleaned_text = \"\"\n",
    "        headtext = [section['headtext'].encode(\"ascii\", \"ignore\").decode().strip()]\n",
    "        cleaned_paragraph = [paragraph.encode(\"ascii\", \"ignore\").decode().strip() for paragraph in section['paragraphs']]\n",
    "        cleaned_text += \". \".join(headtext + cleaned_paragraph)        \n",
    "        \n",
    "        if (len(cleaned_text) < chars_per_section):\n",
    "            cleaned_sections.append(cleaned_text[:len(cleaned_text)])\n",
    "        else:\n",
    "            last_space_index = cleaned_text[:chars_per_section].rfind(' ')\n",
    "            cleaned_sections.append(cleaned_text[:last_space_index])  # last element that is a space\n",
    "\n",
    "    cleaned_sections = '. '.join(cleaned_sections)\n",
    "\n",
    "    return cleaned_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clean_up_strings():\n",
    "    # Test the function with a basic scenario\n",
    "    max_len = 50\n",
    "    sections = [\n",
    "        {\n",
    "            'headtext': \"Sample Headline\",\n",
    "            'paragraphs': [\"This is the first paragraph.\"]\n",
    "        },\n",
    "        {\n",
    "            'headtext': \"Sample Headline\",\n",
    "            'paragraphs': [\"Second paragraph.\"]\n",
    "        }\n",
    "    ]\n",
    "    cleaned_sections = clean_up_strings(max_len, sections)    \n",
    "    expected_result = 'Sample Headline. This is. Sample Headline. Second'    \n",
    "    assert cleaned_sections == expected_result\n",
    "\n",
    "test_clean_up_strings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df.sections.map(lambda x: clean_up_strings(MAX_LENGTH, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentId</th>\n",
       "      <th>postures</th>\n",
       "      <th>sections</th>\n",
       "      <th>label_ids</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ib4e590e0a55f11e8a5d58a2c8dcb28b5</td>\n",
       "      <td>[On Appeal]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Plaintiff Dw...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>. Plaintiff Dwight Watson (Husband) appeals fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ib06ab4d056a011e98c7a8e995225dbf9</td>\n",
       "      <td>[Appellate Review, Sentencing or Penalty Phase...</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['After pleadi...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>. After pleading guilty, William Jerome Howard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iaa3e3390b93111e9ba33b03ae9101fb2</td>\n",
       "      <td>[Motion to Compel Arbitration, On Appeal]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Frederick Gr...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>. Frederick Greene, the plaintiff below, deriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I0d4dffc381b711e280719c3f0e80bdd0</td>\n",
       "      <td>[On Appeal, Review of Administrative Decision]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Appeal from ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>. Appeal from an amended judgment of the Supre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I82c7ef10d6d111e8aec5b23c3317c9c0</td>\n",
       "      <td>[On Appeal]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': ['Order, Supre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>. Order, Supreme Court, New York County (Arthu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Iafe9e30074ba11e88be5ff0f408d813f</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'headtext': 'OPINION &amp; ORDER', 'paragraphs':...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>OPINION &amp; ORDER. The Grievance Committee for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Icfcdb0e00bed11ea83e6f815c7cdf150</td>\n",
       "      <td>[Appellate Review, Sentencing or Penalty Phase...</td>\n",
       "      <td>[{'headtext': 'OPINION', 'paragraphs': ['In 20...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>OPINION. In 2017, a jury convicted Jose Carlos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I0c356d10cfce11e79fcefd9d4766cbba</td>\n",
       "      <td>[Motion to Dismiss]</td>\n",
       "      <td>[{'headtext': 'ORDER OF DISMISSAL', 'paragraph...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>ORDER OF DISMISSAL. BACKGROUND. Plaintiff U.S....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I53552890fb1e11e790b3a4cf54beb9bd</td>\n",
       "      <td>[On Appeal]</td>\n",
       "      <td>[{'headtext': 'SUMMARY ORDER', 'paragraphs': [...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>SUMMARY ORDER. Petitioner-appellant Chauncey M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I916de920dad811e7929ecf6e705a87cd</td>\n",
       "      <td>[On Appeal]</td>\n",
       "      <td>[{'headtext': '', 'paragraphs': [' Plaintiffs ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>. Plaintiffs appeal a judgment dismissing a. F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          documentId  \\\n",
       "0  Ib4e590e0a55f11e8a5d58a2c8dcb28b5   \n",
       "1  Ib06ab4d056a011e98c7a8e995225dbf9   \n",
       "2  Iaa3e3390b93111e9ba33b03ae9101fb2   \n",
       "3  I0d4dffc381b711e280719c3f0e80bdd0   \n",
       "4  I82c7ef10d6d111e8aec5b23c3317c9c0   \n",
       "5  Iafe9e30074ba11e88be5ff0f408d813f   \n",
       "6  Icfcdb0e00bed11ea83e6f815c7cdf150   \n",
       "7  I0c356d10cfce11e79fcefd9d4766cbba   \n",
       "8  I53552890fb1e11e790b3a4cf54beb9bd   \n",
       "9  I916de920dad811e7929ecf6e705a87cd   \n",
       "\n",
       "                                            postures  \\\n",
       "0                                        [On Appeal]   \n",
       "1  [Appellate Review, Sentencing or Penalty Phase...   \n",
       "2          [Motion to Compel Arbitration, On Appeal]   \n",
       "3     [On Appeal, Review of Administrative Decision]   \n",
       "4                                        [On Appeal]   \n",
       "5                                                 []   \n",
       "6  [Appellate Review, Sentencing or Penalty Phase...   \n",
       "7                                [Motion to Dismiss]   \n",
       "8                                        [On Appeal]   \n",
       "9                                        [On Appeal]   \n",
       "\n",
       "                                            sections  \\\n",
       "0  [{'headtext': '', 'paragraphs': ['Plaintiff Dw...   \n",
       "1  [{'headtext': '', 'paragraphs': ['After pleadi...   \n",
       "2  [{'headtext': '', 'paragraphs': ['Frederick Gr...   \n",
       "3  [{'headtext': '', 'paragraphs': ['Appeal from ...   \n",
       "4  [{'headtext': '', 'paragraphs': ['Order, Supre...   \n",
       "5  [{'headtext': 'OPINION & ORDER', 'paragraphs':...   \n",
       "6  [{'headtext': 'OPINION', 'paragraphs': ['In 20...   \n",
       "7  [{'headtext': 'ORDER OF DISMISSAL', 'paragraph...   \n",
       "8  [{'headtext': 'SUMMARY ORDER', 'paragraphs': [...   \n",
       "9  [{'headtext': '', 'paragraphs': [' Plaintiffs ...   \n",
       "\n",
       "                        label_ids  \\\n",
       "0  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "1  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "2  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "3  [0.0, 0.0, 0.0, 1.0, 1.0, 0.0]   \n",
       "4  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "6  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "7  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "8  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "9  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  . Plaintiff Dwight Watson (Husband) appeals fr...  \n",
       "1  . After pleading guilty, William Jerome Howard...  \n",
       "2  . Frederick Greene, the plaintiff below, deriv...  \n",
       "3  . Appeal from an amended judgment of the Supre...  \n",
       "4  . Order, Supreme Court, New York County (Arthu...  \n",
       "5  OPINION & ORDER. The Grievance Committee for t...  \n",
       "6  OPINION. In 2017, a jury convicted Jose Carlos...  \n",
       "7  ORDER OF DISMISSAL. BACKGROUND. Plaintiff U.S....  \n",
       "8  SUMMARY ORDER. Petitioner-appellant Chauncey M...  \n",
       "9  . Plaintiffs appeal a judgment dismissing a. F...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at poltextlab/xlm-roberta-large-english-legal-cap and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([22, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([22]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('poltextlab/xlm-roberta-large-english-legal-cap',\n",
    "                                                           num_labels= len(mlb.classes_),\n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    encoding = tokenizer(examples[\"cleaned_text\"], padding=\"max_length\", truncation=True, max_length=512)    \n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 623.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = Dataset.from_pandas(df)\n",
    "tokenized_datasets = train_dataset.map(tokenize)\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2, seed=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 9/9 [01:46<00:00, 11.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 106.012, 'train_samples_per_second': 0.226, 'train_steps_per_second': 0.085, 'train_loss': 0.4859818352593316, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=0.4859818352593316, metrics={'train_runtime': 106.012, 'train_samples_per_second': 0.226, 'train_steps_per_second': 0.085, 'train_loss': 0.4859818352593316, 'epoch': 3.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=3,\n",
    "    output_dir='./output', \n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "results = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.argmax(results.predictions, axis=1)\n",
    "n_values = len(mlb.classes_)\n",
    "prediction = np.eye(n_values)[values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true=tokenized_datasets[\"test\"]['label_ids'], y_pred=prediction, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a Baseline\n",
    "We noticed from 1st question\n",
    "- Most common class: Appellate Review\n",
    "- Most common number of labels: 1\n",
    "\n",
    "Therefore, the baseline would be to predict everything with \"Appellate Review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred = mlb.transform([['Appellate Review']] * len(tokenized_datasets[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[2, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[2, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[2, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[2, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [1, 0]]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_true=tokenized_datasets[\"test\"]['label_ids'], y_pred=baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true=tokenized_datasets[\"test\"]['label_ids'], y_pred=baseline_pred, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

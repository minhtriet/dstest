{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Supervised fine-tuning (SFT)\n",
    "2. Annotating data with preference labels\n",
    "3. Training a reward model on the preference data\n",
    "4. RL optmization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging to huggingface-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "accelerate>=0.20.3 is required for a normal functioning of this module, but found accelerate==0.16.0.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/code/dstest/venv/lib/python3.9/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[39m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     logging,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     50\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/code/dstest/venv/lib/python3.9/site-packages/transformers/dependency_versions_check.py:57\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     55\u001b[0m             \u001b[39mcontinue\u001b[39;00m  \u001b[39m# not required, check version only if installed\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     require_version_core(deps[pkg])\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m{\u001b[39;00mpkg\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00mdeps\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m, check dependency_versions_table.py\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/dstest/venv/lib/python3.9/site-packages/transformers/utils/versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m hint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTry: pip install transformers -U or pip install -e \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.[dev]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m if you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre working with git main\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m require_version(requirement, hint)\n",
      "File \u001b[0;32m~/code/dstest/venv/lib/python3.9/site-packages/transformers/utils/versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m want_ver \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mfor\u001b[39;00m op, want_ver \u001b[39min\u001b[39;00m wanted\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 111\u001b[0m         _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)\n",
      "File \u001b[0;32m~/code/dstest/venv/lib/python3.9/site-packages/transformers/utils/versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to compare versions for \u001b[39m\u001b[39m{\u001b[39;00mrequirement\u001b[39m}\u001b[39;00m\u001b[39m: need=\u001b[39m\u001b[39m{\u001b[39;00mwant_ver\u001b[39m}\u001b[39;00m\u001b[39m found=\u001b[39m\u001b[39m{\u001b[39;00mgot_ver\u001b[39m}\u001b[39;00m\u001b[39m. This is unusual. Consider\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m reinstalling \u001b[39m\u001b[39m{\u001b[39;00mpkg\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ops[op](version\u001b[39m.\u001b[39mparse(got_ver), version\u001b[39m.\u001b[39mparse(want_ver)):\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrequirement\u001b[39m}\u001b[39;00m\u001b[39m is required for a normal functioning of this module, but found \u001b[39m\u001b[39m{\u001b[39;00mpkg\u001b[39m}\u001b[39;00m\u001b[39m==\u001b[39m\u001b[39m{\u001b[39;00mgot_ver\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhint\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: accelerate>=0.20.3 is required for a normal functioning of this module, but found accelerate==0.16.0.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/chaum/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5733bf84d058e614000b61be</td>\n",
       "      <td>As at most other universities, Notre Dame's st...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5733bed24776f41900661188</td>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5733a6424776f41900660f51</td>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>All of Notre Dame's undergraduate students are...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0   5733be284776f41900661182   \n",
       "5   5733bf84d058e614000b61be   \n",
       "10  5733bed24776f41900661188   \n",
       "15  5733a6424776f41900660f51   \n",
       "20  5733a70c4776f41900660f64   \n",
       "\n",
       "                                              context  \\\n",
       "0   Architecturally, the school has a Catholic cha...   \n",
       "5   As at most other universities, Notre Dame's st...   \n",
       "10  The university is the major seat of the Congre...   \n",
       "15  The College of Engineering was established in ...   \n",
       "20  All of Notre Dame's undergraduate students are...   \n",
       "\n",
       "                       title  \n",
       "0   University_of_Notre_Dame  \n",
       "5   University_of_Notre_Dame  \n",
       "10  University_of_Notre_Dame  \n",
       "15  University_of_Notre_Dame  \n",
       "20  University_of_Notre_Dame  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('squad', split='train')\n",
    "data = data.to_pandas()[['id', 'context', 'title']]\n",
    "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
    "data.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import GPTVectorStoreIndex, StorageContext, ServiceContext\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/chaum/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5733bf84d058e614000b61be</td>\n",
       "      <td>As at most other universities, Notre Dame's st...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5733bed24776f41900661188</td>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5733a6424776f41900660f51</td>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>All of Notre Dame's undergraduate students are...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0   5733be284776f41900661182   \n",
       "5   5733bf84d058e614000b61be   \n",
       "10  5733bed24776f41900661188   \n",
       "15  5733a6424776f41900660f51   \n",
       "20  5733a70c4776f41900660f64   \n",
       "\n",
       "                                              context  \\\n",
       "0   Architecturally, the school has a Catholic cha...   \n",
       "5   As at most other universities, Notre Dame's st...   \n",
       "10  The university is the major seat of the Congre...   \n",
       "15  The College of Engineering was established in ...   \n",
       "20  All of Notre Dame's undergraduate students are...   \n",
       "\n",
       "                       title  \n",
       "0   University_of_Notre_Dame  \n",
       "5   University_of_Notre_Dame  \n",
       "10  University_of_Notre_Dame  \n",
       "15  University_of_Notre_Dame  \n",
       "20  University_of_Notre_Dame  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('squad', split='train')\n",
    "data = data.to_pandas()[['id', 'context', 'title']]\n",
    "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Llama-2-7b-chat-hf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03489468a38c479a82a57ccb76b5cebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dd7b94b5bb410db39a22126a724557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8199eb0816114ccc9518dc637f4bff8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)-00002.safetensors\";:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f1d766e8bf46539ac121a4cc39cdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)-00002.safetensors\";:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbedding(model_name=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs = []\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    docs.append(Document(\n",
    "        text=row['context'],\n",
    "        doc_id=row['id'],\n",
    "        extra_info={'title': row['title']}\n",
    "    ))\n",
    "docs[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List, Optional\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from datasets import Features, Sequence, Value, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    DPRContextEncoder,\n",
    "    DPRContextEncoderTokenizerFast,\n",
    "    HfArgumentParser,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def split_text(text: str, n=100, character=\" \") -> List[str]:\n",
    "    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "\n",
    "def split_documents(documents: dict) -> dict:\n",
    "    \"\"\"Split documents into passages\"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"title\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "\n",
    "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n",
    "    \"\"\"Compute the DPR embeddings of document passages\"\"\"\n",
    "    input_ids = ctx_tokenizer(\n",
    "        documents[\"title\"], documents[\"text\"], truncation=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n",
    "    return {\"embeddings\": embeddings.detach().cpu().numpy()}\n",
    "\n",
    "\n",
    "def main(\n",
    "    rag_example_args: \"RagExampleArguments\",\n",
    "    processing_args: \"ProcessingArguments\",\n",
    "    index_hnsw_args: \"IndexHnswArguments\",\n",
    "):\n",
    "    ######################################\n",
    "    logger.info(\"Step 1 - Create the dataset\")\n",
    "    ######################################\n",
    "\n",
    "    # The dataset needed for RAG must have three columns:\n",
    "    # - title (string): title of the document\n",
    "    # - text (string): text of a passage of the document\n",
    "    # - embeddings (array of dimension d): DPR representation of the passage\n",
    "\n",
    "    # Let's say you have documents in tab-separated csv files with columns \"title\" and \"text\"\n",
    "    assert os.path.isfile(rag_example_args.csv_path), \"Please provide a valid path to a csv file\"\n",
    "\n",
    "    # You can load a Dataset object this way\n",
    "    dataset = load_dataset(\n",
    "        \"csv\", data_files=[rag_example_args.csv_path], split=\"train\", delimiter=\"\\t\", column_names=[\"title\", \"text\"]\n",
    "    )\n",
    "\n",
    "    # More info about loading csv files in the documentation: https://huggingface.co/docs/datasets/loading_datasets.html?highlight=csv#csv-files\n",
    "\n",
    "    # Then split the documents into passages of 100 words\n",
    "    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n",
    "\n",
    "    # And compute the embeddings\n",
    "    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n",
    "    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n",
    "    new_features = Features(\n",
    "        {\"text\": Value(\"string\"), \"title\": Value(\"string\"), \"embeddings\": Sequence(Value(\"float32\"))}\n",
    "    )  # optional, save as float32 instead of float64 to save space\n",
    "    dataset = dataset.map(\n",
    "        partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=processing_args.batch_size,\n",
    "        features=new_features,\n",
    "    )\n",
    "\n",
    "    # And finally save your dataset\n",
    "    passages_path = os.path.join(rag_example_args.output_dir, \"my_knowledge_dataset\")\n",
    "    dataset.save_to_disk(passages_path)\n",
    "    # from datasets import load_from_disk\n",
    "    # dataset = load_from_disk(passages_path)  # to reload the dataset\n",
    "\n",
    "    ######################################\n",
    "    logger.info(\"Step 2 - Index the dataset\")\n",
    "    ######################################\n",
    "\n",
    "    # Let's use the Faiss implementation of HNSW for fast approximate nearest neighbor search\n",
    "    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n",
    "    dataset.add_faiss_index(\"embeddings\", custom_index=index)\n",
    "\n",
    "    # And save the index\n",
    "    index_path = os.path.join(rag_example_args.output_dir, \"my_knowledge_dataset_hnsw_index.faiss\")\n",
    "    dataset.get_index(\"embeddings\").save(index_path)\n",
    "    # dataset.load_faiss_index(\"embeddings\", index_path)  # to reload the index\n",
    "\n",
    "    ######################################\n",
    "    logger.info(\"Step 3 - Load RAG\")\n",
    "    ######################################\n",
    "\n",
    "    # Easy way to load the model\n",
    "    retriever = RagRetriever.from_pretrained(\n",
    "        rag_example_args.rag_model_name, index_name=\"custom\", indexed_dataset=dataset\n",
    "    )\n",
    "    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n",
    "    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n",
    "\n",
    "    # For distributed fine-tuning you'll need to provide the paths instead, as the dataset and the index are loaded separately.\n",
    "    # retriever = RagRetriever.from_pretrained(rag_model_name, index_name=\"custom\", passages_path=passages_path, index_path=index_path)\n",
    "\n",
    "    ######################################\n",
    "    logger.info(\"Step 4 - Have fun\")\n",
    "    ######################################\n",
    "\n",
    "    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n",
    "    input_ids = tokenizer.question_encoder(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    generated = model.generate(input_ids)\n",
    "    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "    logger.info(\"Q: \" + question)\n",
    "    logger.info(\"A: \" + generated_string)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RagExampleArguments:\n",
    "    csv_path: str = field(\n",
    "        default=str(Path(__file__).parent / \"test_data\" / \"my_knowledge_dataset.csv\"),\n",
    "        metadata={\"help\": \"Path to a tab-separated csv file with columns 'title' and 'text'\"},\n",
    "    )\n",
    "    question: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Question that is passed as input to RAG. Default is 'What does Moses' rod turn into ?'.\"},\n",
    "    )\n",
    "    rag_model_name: str = field(\n",
    "        default=\"facebook/rag-sequence-nq\",\n",
    "        metadata={\"help\": \"The RAG model to use. Either 'facebook/rag-sequence-nq' or 'facebook/rag-token-nq'\"},\n",
    "    )\n",
    "    dpr_ctx_encoder_model_name: str = field(\n",
    "        default=\"facebook/dpr-ctx_encoder-multiset-base\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The DPR context encoder model to use. Either 'facebook/dpr-ctx_encoder-single-nq-base' or\"\n",
    "                \" 'facebook/dpr-ctx_encoder-multiset-base'\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    output_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to a directory where the dataset passages and the index will be saved\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingArguments:\n",
    "    num_proc: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The number of processes to use to split the documents into passages. Default is single process.\"\n",
    "        },\n",
    "    )\n",
    "    batch_size: int = field(\n",
    "        default=16,\n",
    "        metadata={\n",
    "            \"help\": \"The batch size to use when computing the passages embeddings using the DPR context encoder.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IndexHnswArguments:\n",
    "    d: int = field(\n",
    "        default=768,\n",
    "        metadata={\"help\": \"The dimension of the embeddings to pass to the HNSW Faiss index.\"},\n",
    "    )\n",
    "    m: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The number of bi-directional links created for every new element during the HNSW index construction.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.WARNING)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    parser = HfArgumentParser((RagExampleArguments, ProcessingArguments, IndexHnswArguments))\n",
    "    rag_example_args, processing_args, index_hnsw_args = parser.parse_args_into_dataclasses()\n",
    "    with TemporaryDirectory() as tmp_dir:\n",
    "        rag_example_args.output_dir = rag_example_args.output_dir or tmp_dir\n",
    "        main(rag_example_args, processing_args, index_hnsw_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Supervised fine-tuning (SFT)\n",
    "2. Annotating data with preference labels\n",
    "3. Training a reward model on the preference data\n",
    "4. RL optmization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging to huggingface-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('squad', split='train')\n",
    "data = data.to_pandas()[['id', 'context', 'title']]\n",
    "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
    "data.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def return_prompt_and_responses(samples) -> Dict[str, str, str]:\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            for question in samples[\"question\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"response_j\"],   # rated better than k\n",
    "        \"rejected\": samples[\"response_k\"], # rated worse than j\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"lvwerra/stack-exchange-paired\",\n",
    "    split=\"train\",\n",
    "    data_dir=\"data/rl\"\n",
    ")\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    remove_columns=original_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervise finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,        # \"meta-llama/Llama-2-7b-hf\"\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# add LoRA layers on top of the quantized base model\n",
    "peft_config = LoraConfig(\n",
    "    r=script_args.lora_r,\n",
    "    lora_alpha=script_args.lora_alpha,\n",
    "    lora_dropout=script_args.lora_dropout,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Supervised fine-tuning (SFT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    packing=True,\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,         # HF Trainer arguments\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\qa.ipynb Cell 22\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mpipeline(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mI liked \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBreaking Bad\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBand of Brothers\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. Do you have any recommendations of other shows I might like?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:834\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 834\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    835\u001b[0m         model,\n\u001b[0;32m    836\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    837\u001b[0m         config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    838\u001b[0m         framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    839\u001b[0m         task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    840\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    841\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    842\u001b[0m     )\n\u001b[0;32m    844\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    845\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:220\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    221\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    226\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_pipeline\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model,                 # base model from SFT pipeline\n",
    "    model_ref,             # typically a copy of the SFT trained base model\n",
    "    beta=0.1,              # temperature hyperparameter of DPO\n",
    "    train_dataset=dataset, # dataset prepared above\n",
    "    tokenizer=tokenizer,   # tokenizer\n",
    "    args=training_args,    # training arguments e.g. batch size, lr, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, StorageContext, ServiceContext\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'trailing_sep_maybe_asterisk' from 'fsspec.implementations.local' (c:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\fsspec\\implementations\\local.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\qa.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39msquad\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ext.minh.triet.chau/code/dstest/qa.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto_pandas()[[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\__init__.py:22\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[0;32m     20\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2.14.5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marrow_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marrow_reader\u001b[39;00m \u001b[39mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[0;32m     66\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marrow_reader\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrowReader\n\u001b[1;32m---> 67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marrow_writer\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata_files\u001b[39;00m \u001b[39mimport\u001b[39;00m sanitize_patterns\n\u001b[0;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdownload\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownload_config\u001b[39;00m \u001b[39mimport\u001b[39;00m DownloadConfig\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\arrow_writer.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpq\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m \u001b[39mimport\u001b[39;00m Features, Image, Value\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     FeatureType,\n\u001b[0;32m     30\u001b[0m     _ArrayXDExtensionType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     to_pyarrow_listarray,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfilesystems\u001b[39;00m \u001b[39mimport\u001b[39;00m is_remote_filesystem\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\features\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mAudio\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mArray2D\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTranslationVariableLanguages\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m \u001b[39mimport\u001b[39;00m Audio\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m \u001b[39mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\features\\audio.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownload\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownload_config\u001b[39;00m \u001b[39mimport\u001b[39;00m DownloadConfig\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownload\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstreaming_download_manager\u001b[39;00m \u001b[39mimport\u001b[39;00m xopen, xsplitext\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtable\u001b[39;00m \u001b[39mimport\u001b[39;00m array_cast\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpy_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m no_op_if_value_is_null, string_to_dict\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\download\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdownload_config\u001b[39;00m \u001b[39mimport\u001b[39;00m DownloadConfig\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdownload_manager\u001b[39;00m \u001b[39mimport\u001b[39;00m DownloadManager, DownloadMode\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstreaming_download_manager\u001b[39;00m \u001b[39mimport\u001b[39;00m StreamingDownloadManager\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maiohttp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient_exceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m ClientError\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfilesystems\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPRESSION_FILESYSTEMS\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfile_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     get_authentication_headers_for_url,\n\u001b[0;32m     24\u001b[0m     get_datasets_user_agent,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     url_or_path_join,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m get_logger\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\datasets\\filesystems\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39masyn\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compression\n\u001b[0;32m     13\u001b[0m _has_s3fs \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mfind_spec(\u001b[39m\"\u001b[39m\u001b[39ms3fs\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\fsspec\\asyn.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m _DEFAULT_CALLBACK\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m FSTimeoutError\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimplementations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlocal\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     LocalFileSystem,\n\u001b[0;32m     18\u001b[0m     make_path_posix,\n\u001b[0;32m     19\u001b[0m     trailing_sep,\n\u001b[0;32m     20\u001b[0m     trailing_sep_maybe_asterisk,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mspec\u001b[39;00m \u001b[39mimport\u001b[39;00m AbstractBufferedFile, AbstractFileSystem\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_exception, other_paths\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'trailing_sep_maybe_asterisk' from 'fsspec.implementations.local' (c:\\Users\\ext.minh.triet.chau\\code\\dstest\\venv\\lib\\site-packages\\fsspec\\implementations\\local.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('squad', split='train')\n",
    "data = data.to_pandas()[['id', 'context', 'title']]\n",
    "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs = []\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    docs.append(Document(\n",
    "        text=row['context'],\n",
    "        doc_id=row['id'],\n",
    "        extra_info={'title': row['title']}\n",
    "    ))\n",
    "docs[0]\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
